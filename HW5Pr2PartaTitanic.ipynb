{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'female', 'male', 'female', 'female', 'female', 'female', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'female', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female', '', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male']\n",
      "2 [109, 109, 109, 109, 109, 109, 102, 109, 109, 102, 102, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 109, 102, 102, 109, 102, 109, 109, 109, 102, 109, 109, 109, 102, 109, 102, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 102, 109, 102, 102, 102, 109, 109, 109, 102, 102, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 102, 102, 109, 109, 109, 102, 109, 109, 102, 109, 109, 102, 109, 102, 109, 102, 102, 109, 109, 102, 109, 102, 109, 109, 102, 109, 109, 109, 109, 102, 109, 102, 102, 109, 109, 102, 102, 109, 109, 102, 102, 109, 109, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 102, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 102, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 109, 102, 102, 109, 109, 109, 102, 102, 109, 109, 102, 109, 102, 102, 109, 102, 102, 109, 109, 102, 109, 109, 102, 102, 102, 102, 109, 102, 102, 109, 102, 109, 109, 102, 102, 109, 102, 102, 109, 102, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 102, 109, 109, 102, 102, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 102, 102, 109, 109, 102, 109, 109, 109, 102, 102, 109, 102, 109, 102, 102, 102, 109, 102, 109, 102, 102, 102, 102, 109, 102, 102, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 102, 102, 102, 102, 102, 109, 109, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 102, 102, 109, 102, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 102, 109, 109, 109, 102, 109, 102, 102, 109, 109, 109, 109, 102, 102, 102, 102, 109, 109, 109, 102, 109, 102, 102, 109, 102, 109, 109, 102, 102, 109, 109, 109, 102, 109, 102, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 109, 102, 109, 109, 109, 109, 102, 109, 109, 102, 109, 102, 102, 109, 109, 102, 102, 109, 109, 109, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 102, 109, 109, 102, 102, 109, 102, 109, 102, 109, 102, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 102, 109, 102, 102, 102, 109, 102, 109, 102, 102, 102, 102, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 102, 102, 109, 109, 102, 109, 109, 109, 102, 102, 102, 109, 109, 109, 102, 109, 102, 109, 102, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 102, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 102, 102, 102, 109, 102, 109, 102, 109, 109, 109, 109, 102, 109, 102, 109, 109, 102, 109, 102, 109, 102, 109, 102, 109, 109, 109, 102, 109, 109, 109, 102, 109, 102, 102, 109, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 102, 109, 102, 109, 102, 109, 109, 109, 102, 109, 102, 109, 109, 102, 102, 109, 102, 102, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 102, 102, 109, 102, 102, 109, 102, 102, 102, 102, 102, 109, 109, 109, 102, 109, 102, 109, 102, 102, 102, 102, 109, 109, 109, 109, 109, 102, 109, 102, 109, 102, 109, 102, 102, 109, 109, 102, 109, 109, 102, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 102, 109, 102, 109, 102, 102, 109, 102, 109, 109, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 102, 109, 102, 102, 109, 102, 109, 102, 109, 102, 109, 102, 109, 109, 102, 109, 109, 109, 102, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 109, 109, 109, 102, 109, 109, 109, 102, 102, 102, 109, 102, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 102, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 102, 102, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 102, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 102, 102, 109, 109, 102, 102, 109, 109, 102, 102, 109, 109, 109, 109, 102, 102, 109, 109, 102, 109, 109, 109, 109, 102, 102, 102, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 102, 102, 109, 109, 109, 109, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 102, 109, 109, 102, 109, 109, 102, 102, 102, 109, 109, 102, 102, 102, 109, 109, 109, 102, 109]\n",
      "9 ['S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'Q', 'C', 'Q', 'S', 'C', 'S', 'C', 'Q', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'C', 'Q', 'C', 'S', 'C', 'Q', 'S', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'Q', 'S', 'C', 'S', 'Q', 'S', 'C', 'C', 'S', 'C', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', '', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'C', 'C', 'S', 'S', 'S', 'Q', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'Q', 'S', 'Q', '', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'Q', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S', 'S', 'Q', 'S', 'S', 'C', 'Q', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'Q', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'Q', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'Q', 'Q', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'C', 'C', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'C', 'S', 'S', 'Q', 'Q', 'Q', 'Q', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'C', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'Q', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'Q', 'S', 'C', 'C', 'S', 'C', 'S', 'Q', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', '', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'Q', 'S', 'C', 'C', 'S', 'C', 'S', 'Q', 'Q', 'S', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'Q', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'C', 'Q', 'S', 'S', 'Q', 'Q', 'C', 'C', 'Q', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'C', 'C', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'Q', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'S', 'Q', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'C', 'S', 'S', 'C', 'Q', 'C', 'S', 'Q', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S']\n",
      "9 [83, 67, 67, 83, 83, 83, 83, 83, 83, 67, 83, 67, 67, 83, 67, 67, 83, 83, 83, 83, 83, 83, 67, 67, 83, 67, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 83, 81, 83, 83, 83, 81, 83, 83, 83, 81, 67, 81, 83, 67, 83, 67, 81, 67, 83, 83, 83, 67, 83, 83, 67, 83, 83, 67, 83, 83, 83, 83, 81, 83, 67, 83, 83, 67, 83, 81, 83, 83, 83, 83, 83, 81, 83, 67, 83, 83, 67, 83, 83, 83, 81, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 67, 81, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 67, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 67, 81, 67, 83, 67, 81, 83, 83, 83, 67, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 81, 83, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 83, 67, 67, 81, 83, 67, 83, 81, 83, 67, 67, 83, 67, 67, 83, 83, 67, 83, 83, 67, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 67, 83, 83, 83, 83, 83, 83, 83, 81, 83, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 83, 67, 67, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 83, 81, 83, 67, 67, 83, 83, 83, 81, 81, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 81, 83, 81, 83, 83, 67, 83, 83, 83, 83, 67, 81, 83, 83, 67, 83, 83, 83, 83, 81, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 81, 83, 83, 83, 67, 83, 83, 67, 67, 83, 83, 81, 83, 83, 67, 81, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 67, 67, 83, 83, 67, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 81, 67, 83, 83, 83, 83, 83, 83, 83, 67, 83, 67, 67, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 67, 67, 83, 83, 83, 67, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 83, 67, 81, 83, 83, 83, 83, 83, 67, 81, 83, 67, 83, 83, 81, 83, 83, 67, 83, 81, 67, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 81, 81, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 81, 83, 83, 67, 67, 83, 83, 83, 83, 67, 83, 67, 67, 83, 83, 67, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 67, 67, 67, 83, 67, 83, 83, 67, 83, 83, 83, 81, 67, 83, 83, 81, 81, 81, 81, 81, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 81, 83, 83, 83, 83, 67, 81, 83, 67, 67, 83, 67, 83, 83, 83, 83, 81, 83, 83, 83, 81, 83, 83, 83, 83, 83, 67, 67, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 67, 67, 67, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 67, 83, 83, 81, 81, 81, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 83, 81, 83, 67, 83, 81, 83, 67, 67, 83, 67, 83, 81, 67, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 67, 83, 83, 83, 83, 83, 81, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 81, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 81, 83, 83, 67, 83, 67, 83, 83, 81, 83, 83, 67, 83, 83, 83, 83, 83, 81, 83, 83, 81, 83, 67, 67, 83, 67, 83, 81, 81, 83, 67, 83, 67, 67, 83, 83, 83, 81, 83, 83, 81, 83, 83, 83, 83, 67, 81, 67, 83, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 81, 67, 81, 83, 83, 81, 81, 67, 67, 81, 67, 83, 83, 83, 83, 83, 83, 67, 67, 67, 67, 83, 83, 83, 83, 83, 83, 83, 67, 83, 83, 83, 83, 67, 67, 67, 67, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 81, 83, 67, 81, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 83, 83, 83, 83, 67, 83, 67, 67, 83, 83, 83, 83, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 81, 83, 83, 83, 81, 67, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 67, 83, 67, 83, 67, 83, 83, 67, 81, 67, 83, 81, 83, 67, 83, 83, 83, 81, 83, 67, 81, 83, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83]\n",
      "1 ['female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'female', 'female', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'female', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'female', 'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male']\n",
      "1 [102, 102, 109, 109, 109, 102, 109, 109, 109, 109, 102, 102, 102, 102, 102, 102, 102, 109, 102, 109, 109, 109, 102, 102, 109, 109, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 102, 109, 109, 109, 102, 109, 109, 109, 109, 109, 102, 102, 109, 102, 109, 109, 109, 102, 102, 109, 102, 102, 109, 102, 102, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 102, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 102, 109, 109, 102, 109, 109, 109, 102, 109, 109, 109, 102, 102, 102, 102, 109, 102, 109, 109, 102, 109, 102, 102, 102, 109, 109, 102, 102, 102, 109, 102, 102, 102, 109, 109, 109, 109, 102, 109, 109, 109, 109, 109, 109, 102, 109, 109, 102, 102, 102, 109, 109, 102, 109, 109, 109, 102, 102, 109, 109, 109, 102, 109, 109, 102, 102, 102, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 109, 109, 109, 102, 102, 109, 102, 102, 102, 109, 109, 102, 109, 109, 102, 109, 102, 109, 109, 102, 109, 109, 102, 109, 102, 109, 109, 102, 109, 109, 109, 102, 102, 102, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 102, 102, 109, 102, 102, 109, 102, 102, 102, 109, 102, 102, 109, 102, 109, 102, 109, 102, 109, 102, 109, 102, 109, 109, 109, 109, 109, 102, 109, 109, 109, 102, 102, 109, 109, 109, 109, 102, 102, 109, 102, 109, 102, 109, 109, 109, 102, 109, 102, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 102, 102, 109, 102, 109, 102, 102, 102, 109, 109, 109, 102, 109, 109, 109]\n",
      "8 ['C', 'C', 'S', 'S', 'S', 'C', 'C', 'S', 'Q', 'C', 'C', 'S', 'C', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'C', 'S', 'Q', 'C', 'S', 'C', 'Q', 'S', 'Q', 'Q', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'Q', 'Q', 'S', 'Q', 'S', 'Q', 'S', 'C', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'Q', 'S', 'Q', 'S', 'Q', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'Q', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'C', 'Q', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'Q', 'C', 'Q', 'C', 'Q', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'Q', 'C', 'S', 'S', 'C', 'S', 'S', 'Q', 'Q']\n",
      "8 [67, 67, 83, 83, 83, 67, 67, 83, 81, 67, 67, 83, 67, 83, 83, 83, 67, 67, 83, 83, 67, 83, 81, 67, 83, 67, 81, 83, 81, 81, 83, 83, 83, 81, 83, 83, 83, 83, 83, 83, 67, 83, 83, 81, 83, 83, 67, 83, 83, 67, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 67, 81, 81, 83, 81, 83, 81, 83, 67, 67, 81, 83, 83, 83, 83, 83, 81, 83, 83, 83, 83, 83, 67, 83, 83, 83, 67, 81, 83, 81, 83, 81, 81, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 83, 83, 67, 83, 83, 67, 83, 67, 67, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 83, 67, 83, 83, 67, 83, 83, 81, 67, 83, 67, 83, 83, 83, 83, 83, 67, 83, 83, 83, 67, 83, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 81, 83, 83, 83, 83, 83, 83, 83, 83, 83, 67, 67, 83, 83, 83, 83, 83, 83, 83, 83, 81, 83, 83, 67, 67, 81, 67, 83, 83, 83, 67, 83, 83, 67, 83, 83, 83, 83, 83, 83, 81, 67, 83, 67, 83, 83, 83, 83, 67, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 83, 67, 83, 83, 83, 83, 81, 67, 83, 67, 83, 83, 83, 83, 83, 83, 83, 67, 67, 81, 67, 81, 67, 81, 83, 83, 83, 67, 83, 83, 67, 81, 83, 83, 67, 83, 83, 83, 81, 67, 67, 83, 83, 83, 83, 67, 83, 83, 67, 83, 83, 67, 67, 83, 83, 83, 83, 67, 83, 81, 67, 83, 83, 67, 83, 83, 81, 81]\n"
     ]
    }
   ],
   "source": [
    "# Run only once\n",
    "\n",
    "# playground for loading titanic\n",
    "# method is quite manual. I will have a handmade array of indicators, indicator will be 1 for a\n",
    "# column that is number, and 0 for a column that is categorical\n",
    "# I will have a separate array, with values indicating whether the data in that class is discreet\n",
    "# or continuous\n",
    "\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import scipy.io\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import math\n",
    "ArrDtype =np.array([1,1,0,1,1,1,0,1,0,0],dtype = 'int32')\n",
    "ArrClass =np.array([1,1,1,0,0,0,0,0,0,1],dtype = 'int32') # 1 for class, 0 for continuous\n",
    "\n",
    "DropFeature = [7,9]\n",
    "path_train = 'datasets/titanic/titanic_training.csv'\n",
    "FeatureSize = 10\n",
    "DataSize = 1000\n",
    "\n",
    "X = np.zeros((DataSize,FeatureSize-len(DropFeature)-1))# 1 is to account for labels\n",
    "k =0\n",
    "for i in range(0,FeatureSize):\n",
    "    if ((i+1) in DropFeature):\n",
    "        continue\n",
    "    if (ArrDtype[i]==1):\n",
    "#data = genfromtxt(path_train, delimiter=',', dtype=str,usecols=8,skip_header=1)\n",
    "        data = genfromtxt(path_train, delimiter=',',usecols=i,skip_header=1)\n",
    "        if(ArrClass[i]==1):\n",
    "            MedianClass = np.nanmedian(np.array(data))\n",
    "            for j in range(0,len(data)):\n",
    "                if math.isnan(data[j]):\n",
    "                    data[j] = MedianClass\n",
    "        else:\n",
    "            MeanClass = np.nanmean(np.array(data))\n",
    "            for j in range(0,len(data)):\n",
    "                if math.isnan(data[j]):\n",
    "                    data[j] = MeanClass\n",
    "    else:\n",
    "        data = list(genfromtxt(path_train, delimiter=',', dtype=str,usecols=i,skip_header=1))\n",
    "        print(i,data)\n",
    "        U =np.unique(data,return_counts = False)\n",
    "        for j in range(0,len(data)):\n",
    "            if(data[j]==''):\n",
    "                data[j]=U[-1]       #for j in range(0,len(data)):\n",
    "            data[j] = ord(data[j][0])  \n",
    "        print(i,data)\n",
    "    if(i>0):\n",
    "        X[:,k] = np.array(data)\n",
    "        k = k+1\n",
    "    else:\n",
    "        y = np.array(data, dtype='int32')\n",
    "# doing the same for test\n",
    "ArrDtype =np.array([1,0,1,1,1,0,1,0,0],dtype = 'int32')\n",
    "ArrClass =np.array([1,1,0,0,0,0,0,0,1],dtype = 'int32') # 1 for class, 0 for continuous\n",
    "\n",
    "DropFeature = [6,8] \n",
    "\n",
    "path_train = 'datasets/titanic/titanic_testing_data.csv'\n",
    "FeatureSize = 9\n",
    "DataSize = 310\n",
    "Z = np.zeros((DataSize,FeatureSize-len(DropFeature)))\n",
    "k =0\n",
    "\n",
    "for i in range(0,FeatureSize):\n",
    "    if ((i+1) in DropFeature):\n",
    "        continue\n",
    "    if (ArrDtype[i]==1):\n",
    "#data = genfromtxt(path_train, delimiter=',', dtype=str,usecols=8,skip_header=1)\n",
    "        data = genfromtxt(path_train, delimiter=',',usecols=i,skip_header=1)\n",
    "        if(ArrClass[i]==1):\n",
    "            MedianClass = np.nanmedian(np.array(data))\n",
    "            for j in range(0,len(data)):\n",
    "                if math.isnan(data[j]):\n",
    "                    data[j] = MedianClass\n",
    "        else:\n",
    "            MeanClass = np.nanmean(np.array(data))\n",
    "            for j in range(0,len(data)):\n",
    "                if math.isnan(data[j]):\n",
    "                    data[j] = MeanClass\n",
    "    else:\n",
    "        data = list(genfromtxt(path_train, delimiter=',', dtype=str,usecols=i,skip_header=1))\n",
    "        #print(i,data)\n",
    "        U =np.unique(data,return_counts = False)\n",
    "        #print(i,data)\n",
    "        for j in range(0,len(data)):\n",
    "            if(data[j]==''):\n",
    "                data[j]=U[-1]       #for j in range(0,len(data)):\n",
    "            data[j] = ord(data[j][0])  \n",
    "        #print(i,data)\n",
    "        #break\n",
    "        #break\n",
    "    \n",
    "    Z[:,k] = np.array(data)\n",
    "    k = k+1\n",
    "scipy.io.savemat('TitanicProcessedV2.mat',{'X':X,'y':y,'Z':Z,'ArrClass':ArrClass})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "5.662372589111328 seconds\n",
      "2.0\n",
      "9.097947597503662 seconds\n",
      "3.0\n",
      "10.764043092727661 seconds\n",
      "4.0\n",
      "12.911655187606812 seconds\n",
      "5.0\n",
      "14.495228290557861 seconds\n",
      "6.0\n",
      "15.604679346084595 seconds\n",
      "7.0\n",
      "16.632066249847412 seconds\n",
      "8.0\n",
      "17.49485754966736 seconds\n",
      "9.0\n",
      "18.833338975906372 seconds\n",
      "10.0\n",
      "19.303059101104736 seconds\n",
      "11.0\n",
      "20.229196548461914 seconds\n",
      "12.0\n",
      "20.96673035621643 seconds\n",
      "13.0\n",
      "20.87158966064453 seconds\n",
      "14.0\n",
      "21.492849349975586 seconds\n",
      "15.0\n",
      "21.947628021240234 seconds\n",
      "16.0\n",
      "22.76578950881958 seconds\n",
      "17.0\n",
      "21.483523845672607 seconds\n",
      "18.0\n",
      "21.858357906341553 seconds\n",
      "19.0\n",
      "21.98302149772644 seconds\n",
      "20.0\n",
      "22.37228775024414 seconds\n",
      "21.0\n",
      "23.734850883483887 seconds\n",
      "22.0\n",
      "23.97972583770752 seconds\n",
      "23.0\n",
      "24.601022005081177 seconds\n",
      "24.0\n",
      "23.641976833343506 seconds\n",
      "25.0\n",
      "23.0635929107666 seconds\n",
      "26.0\n",
      "23.931058168411255 seconds\n",
      "27.0\n",
      "22.915440559387207 seconds\n",
      "28.0\n",
      "23.761109352111816 seconds\n",
      "29.0\n",
      "23.223272800445557 seconds\n",
      "30.0\n",
      "23.28093695640564 seconds\n",
      "31.0\n",
      "23.099812030792236 seconds\n",
      "32.0\n",
      "23.626346588134766 seconds\n",
      "33.0\n",
      "22.805022716522217 seconds\n",
      "34.0\n",
      "23.246593952178955 seconds\n",
      "35.0\n",
      "23.29048991203308 seconds\n",
      "36.0\n",
      "22.783732891082764 seconds\n",
      "37.0\n",
      "25.067588567733765 seconds\n",
      "38.0\n",
      "24.648263931274414 seconds\n",
      "39.0\n",
      "24.36037540435791 seconds\n",
      "40.0\n",
      "23.369725227355957 seconds\n",
      "ValidationError [[0.24 ]\n",
      " [0.265]\n",
      " [0.22 ]\n",
      " [0.22 ]\n",
      " [0.23 ]\n",
      " [0.22 ]\n",
      " [0.225]\n",
      " [0.24 ]\n",
      " [0.24 ]\n",
      " [0.25 ]\n",
      " [0.245]\n",
      " [0.25 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.245]\n",
      " [0.255]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.25 ]\n",
      " [0.265]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]]\n",
      "ValidationError [[0.24 ]\n",
      " [0.265]\n",
      " [0.22 ]\n",
      " [0.22 ]\n",
      " [0.23 ]\n",
      " [0.22 ]\n",
      " [0.225]\n",
      " [0.24 ]\n",
      " [0.24 ]\n",
      " [0.25 ]\n",
      " [0.245]\n",
      " [0.25 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.245]\n",
      " [0.255]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.25 ]\n",
      " [0.265]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the starter code and some suggested architecture we provide you with. \n",
    "But feel free to do any modifications as you wish or just completely ignore \n",
    "all of them and have your own implementations.\n",
    "\"\"\"\n",
    "# some inspirations were taken from \n",
    "# https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(1242)\n",
    "random.seed(1242)\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self,CostMethod,MaxDepth):\n",
    "        self.CostMethod = CostMethod\n",
    "        self.MaxDepth = MaxDepth\n",
    "        \"\"\"\n",
    "        TODO: initialization of a decision tree\n",
    "        \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates the entropy given all the labels\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        # assuming binary class\n",
    "        if (len(y)==0):\n",
    "            H = 0\n",
    "        else:\n",
    "            Pc1 = np.sum(y[y==1])/len(y)\n",
    "            Pc0 = 1-Pc1\n",
    "            if ((Pc1==0) |(Pc0==0)):\n",
    "                H=0\n",
    "            else:\n",
    "                H = - (Pc1*np.log(Pc1)+Pc0*np.log(Pc0))\n",
    "        return H\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates information gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(y):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates the gini impurity given all the labels\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates reduction in impurity gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def split(self,SplitVal,SplitIndex, Data):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that return a split of the dataset given an index of the feature and\n",
    "        a threshold for it\n",
    "        \"\"\"\n",
    "        left,right = list(), list()\n",
    "        for DataRow in Data:\n",
    "            if (DataRow[SplitIndex]<=SplitVal):\n",
    "                left.append(DataRow)\n",
    "            else:\n",
    "                right.append(DataRow)\n",
    "        return left,right\n",
    "\n",
    "    def segmenter(self, Node):\n",
    "        \"\"\"\n",
    "        TODO: compute entropy gain for all single-dimension splits,\n",
    "        return the feature and the threshold for the split that\n",
    "        has maximum gain\n",
    "        \"\"\"\n",
    "        X = np.array(Node['Data'])\n",
    "        U,V = np.shape(X)\n",
    "        Left,Right = self.split(X[0,0],0,X)\n",
    "        if (self.CostMethod =='Entropy'):\n",
    "            Uleft = len(Left)\n",
    "            if(Uleft ==0):\n",
    "                Hleft =0\n",
    "            else:\n",
    "                Hleft = self.entropy(np.array(Left)[:,-1])\n",
    "            Uright = len(Right)\n",
    "            if(Uright ==0):\n",
    "                Hright =0\n",
    "            else:\n",
    "                Hright = self.entropy(np.array(Right)[:,-1])\n",
    "            Cost = (Uright*Hright+Uleft*Hleft)/U\n",
    "        baselineVal,baselineFeatureIndex,baselineCost,baselineLeft,baselineRight=X[0,0],0,Cost,Left,Right\n",
    "        \n",
    "        for i in range(0,U):\n",
    "            for j in range(0,V-1):\n",
    "                Left,Right = self.split(X[i,j],j,X)\n",
    "                if (self.CostMethod =='Entropy'):\n",
    "                    Uleft = len(Left)\n",
    "                    if(Uleft ==0):\n",
    "                        Hleft =0\n",
    "                    else:\n",
    "                        Hleft = self.entropy(np.array(Left)[:,-1])\n",
    "                    Uright = len(Right)\n",
    "                    if(Uright ==0):\n",
    "                        Hright =0\n",
    "                    else:\n",
    "                        Hright = self.entropy(np.array(Right)[:,-1])\n",
    "                    Cost = (Uright*Hright+Uleft*Hleft)/U\n",
    "                if (Cost<baselineCost):\n",
    "                    baselineVal,baselineFeatureIndex,baselineCost,baselineLeft,baselineRight=X[i,j],j,Cost,Left,Right\n",
    "        \n",
    "        Node['Left']   ={'Data':baselineLeft,\n",
    "                         'Label':'Node',\n",
    "                         'Threshold':'nan',\n",
    "                         'Index' : 'nan',\n",
    "                         'Left':'nan',\n",
    "                         'Right' : 'nan',\n",
    "                         'LeafLabel' : 'nan', \n",
    "                        }\n",
    "        Node['Right']  ={'Data':baselineRight,\n",
    "                         'Label':'Node',\n",
    "                         'Threshold':'nan',\n",
    "                         'Index' : 'nan',\n",
    "                         'Left':'nan',\n",
    "                         'Right' : 'nan',\n",
    "                         'LeafLabel' : 'nan', \n",
    "                        }\n",
    "        Node['Threshold'] = baselineVal\n",
    "        Node['Index'] = baselineFeatureIndex\n",
    "        return \n",
    "    def leaf(self,X):\n",
    "        X['Label'] = 'Leaf'\n",
    "        y = np.array(X['Data'])[:,-1]\n",
    "        if (np.count_nonzero(y==1)>np.count_nonzero(y==0)):\n",
    "            X['LeafLabel']=1\n",
    "        else:\n",
    "            X['LeafLabel']=0\n",
    "        return\n",
    "    def CreateBranch(self,X,DepthLevel):\n",
    "        if((DepthLevel+1)>self.MaxDepth):\n",
    "            self.leaf(X)\n",
    "        elif((X['Left']!= 'nan') &(X['Right']!= 'nan') &((len(X['Left'])==0)|(len(X['Right'])==0))):\n",
    "            self.leaf(X)\n",
    "        else:\n",
    "            self.segmenter(X)\n",
    "            if(len(X['Left']['Data'])==0):  # see if there is no segmentation, if so then end the node here\n",
    "                self.leaf(X['Right'])\n",
    "                X['Left']['Label'] = 'Leaf'\n",
    "                X['Left']['LeafLabel'] = X['Right']['LeafLabel']\n",
    "            elif(len(X['Right']['Data'])==0):  # see if there is no segmentation, if so then end the node here\n",
    "                self.leaf(X['Left'])\n",
    "                X['Right']['Label'] = 'Leaf'\n",
    "                X['Right']['LeafLabel'] = X['Left']['LeafLabel']\n",
    "            else:\n",
    "                self.CreateBranch(X['Left'],DepthLevel+1)\n",
    "                self.CreateBranch(X['Right'],DepthLevel+1)\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set. Think about what would be \n",
    "        your stopping criteria\n",
    "        \"\"\"\n",
    "        NodeLevel = 0\n",
    "        Root ={'Data':X,\n",
    "              'Label':'Node',\n",
    "              'Threshold':'nan',\n",
    "              'Index' : 'nan',\n",
    "              'Left':'nan',\n",
    "              'Right' : 'nan',\n",
    "              'LeafLabel' : 'nan', \n",
    "              }\n",
    "        #SplitVal,SplitIndex,LeftGroup,RightGroup = self.segmenter(X,NodeLevel)\n",
    "        self.CreateBranch(Root,NodeLevel)\n",
    "        #self.__repr__(Root,'\\t') # if you want to visualize the tree\n",
    "        return Root\n",
    "\n",
    "    def predict(self, Node,X):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "        if(Node['Label']=='Leaf'):\n",
    "            Label = Node['LeafLabel']\n",
    "        else:\n",
    "            y = Node['Index']\n",
    "            if (X[y]>Node['Threshold']):\n",
    "                Label = self.predict(Node['Right'],X)\n",
    "            else:\n",
    "                Label = self.predict(Node['Left'],X)\n",
    "        return Label\n",
    "\n",
    "    def __repr__(self,Node,tab):\n",
    "        \"\"\"\n",
    "        TODO: one way to visualize the decision tree is to write out a __repr__ method\n",
    "        that returns the string representation of a tree. Think about how to visualize \n",
    "        a tree structure. You might have seen this before in CS61A.\n",
    "        \"\"\"\n",
    "        if(Node['Label']=='Node'):\n",
    "            print(tab,'if X[',Node['Index'],']>',Node['Threshold'])\n",
    "            self.__repr__(Node['Right'],tab+'\\t')\n",
    "            print(tab,'else')\n",
    "            self.__repr__(Node['Left'],tab+'\\t')\n",
    "        else:\n",
    "            print(tab,'y=',Node['LeafLabel'])\n",
    "        return 0\n",
    "\n",
    "def DataSplit(data, labels, val_size):\n",
    "    num_items = len(data)\n",
    "    assert num_items == len(labels)\n",
    "    assert val_size >= 0\n",
    "    if val_size < 1.0:\n",
    "        val_size = int(num_items * val_size)\n",
    "    train_size = num_items - val_size\n",
    "    idx = np.random.permutation(num_items)\n",
    "    data_train1 = data[idx][:train_size]\n",
    "    label_train = labels[idx][:train_size]\n",
    "    data_val = data[idx][train_size:]\n",
    "    label_val = labels[idx][train_size:]\n",
    "    return data_train1, data_val, label_train, label_val\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset = \"titanic\"\n",
    "    dataset = \"titanic\"\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data       \n",
    "        data1 =scipy.io.loadmat('TitanicProcessedV2.mat')\n",
    "        X = data1[\"X\"]\n",
    "        y = data1[\"y\"]\n",
    "        Z = data1[\"Z\"]    \n",
    "        y = y.reshape(np.size(y),1)\n",
    "        # TODO: preprocess titanic dataset\n",
    "        # Notes: \n",
    "        # 1. Some data points are missing their labels\n",
    "        # 2. Some features are not numerical but categorical\n",
    "        # 3. Some values are missing for some features\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam-dataset/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "         \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "    # splitting data into training and validation\n",
    "    ValidationSize = int(len(y)*0.20)\n",
    "    Roots =list()\n",
    "    TrainingData,ValidationData,TrainingLabel,ValidationLabel = DataSplit(X,y,ValidationSize)\n",
    "    # converting the data into array for easier preprocessing\n",
    "    TrainingData = np.array(TrainingData)\n",
    "    TrainingLabel = np.array(TrainingLabel)\n",
    "    ValidationData = np.array(ValidationData)\n",
    "    ValidationLabel = np.array(ValidationLabel)\n",
    "    TrainingData = np.concatenate((TrainingData,TrainingLabel.reshape(len(TrainingLabel),1)),axis=1)\n",
    "    #creating an instance of the classifier\n",
    "    MaxDepth =np.linspace(1,40,40)\n",
    "    ValidationError = np.zeros((len(MaxDepth),1),dtype ='float64')\n",
    "    TrainingError = np.zeros((len(MaxDepth),1),dtype ='float64')\n",
    "    for j in range(0,len(MaxDepth)):\n",
    "        classifier = DecisionTree('Entropy',MaxDepth[j])\n",
    "        print(MaxDepth[j])\n",
    "        import time\n",
    "        start = time.time()\n",
    "        Root= classifier.fit(TrainingData)\n",
    "        ValidationPredicted = np.zeros((ValidationSize,1),dtype ='int64')\n",
    "        TrainingPredicted = np.zeros((len(TrainingLabel),1),dtype ='int64')\n",
    "        for i in range(0,len(ValidationLabel)):\n",
    "            ValidationPredicted[i] = classifier.predict(Root,ValidationData[i,:])\n",
    "            \n",
    "        ValidationError[j] = np.sum(np.abs((ValidationLabel.reshape(ValidationSize,1)\n",
    "                                            -ValidationPredicted.reshape(ValidationSize,1))\n",
    "                                           .reshape(ValidationSize,1)))/ValidationSize\n",
    "        for i in range(0,len(TrainingLabel)):\n",
    "            TrainingPredicted[i] = classifier.predict(Root,TrainingData[i,:])\n",
    "            \n",
    "        TrainingError[j] = np.sum(np.abs((TrainingLabel.reshape(len(TrainingLabel),1)\n",
    "                                            -TrainingPredicted.reshape(len(TrainingLabel),1))\n",
    "                                           .reshape(len(TrainingLabel),1)))/len(TrainingLabel)\n",
    "        end = time.time()\n",
    "        print(end-start,'seconds')\n",
    "        Roots.append(Root)\n",
    "    print('ValidationError',ValidationError)\n",
    "    print('ValidationError',ValidationError)\n",
    "    #print(Root)\n",
    "    scipy.io.savemat('TitanicDTValidationV2.mat',{'ValidationError':ValidationError,'Tree Depth':MaxDepth,'Root':Roots})\n",
    "    \"\"\"\n",
    "    TODO: train decision tree/random forest on different datasets and perform the tasks \n",
    "    in the problem\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingError [[0.20375]\n",
      " [0.20125]\n",
      " [0.17625]\n",
      " [0.17625]\n",
      " [0.1625 ]\n",
      " [0.15875]\n",
      " [0.14875]\n",
      " [0.14125]\n",
      " [0.1375 ]\n",
      " [0.1225 ]\n",
      " [0.1125 ]\n",
      " [0.10625]\n",
      " [0.095  ]\n",
      " [0.08375]\n",
      " [0.08   ]\n",
      " [0.07   ]\n",
      " [0.05875]\n",
      " [0.05125]\n",
      " [0.045  ]\n",
      " [0.0425 ]\n",
      " [0.0375 ]\n",
      " [0.03375]\n",
      " [0.03125]\n",
      " [0.02875]\n",
      " [0.0275 ]\n",
      " [0.02625]\n",
      " [0.025  ]\n",
      " [0.02375]\n",
      " [0.02375]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]\n",
      " [0.0225 ]]\n",
      "ValidationError [[0.24 ]\n",
      " [0.265]\n",
      " [0.22 ]\n",
      " [0.22 ]\n",
      " [0.23 ]\n",
      " [0.22 ]\n",
      " [0.225]\n",
      " [0.24 ]\n",
      " [0.24 ]\n",
      " [0.25 ]\n",
      " [0.245]\n",
      " [0.25 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.245]\n",
      " [0.255]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.255]\n",
      " [0.25 ]\n",
      " [0.265]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.27 ]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.265]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]\n",
      " [0.26 ]]\n"
     ]
    }
   ],
   "source": [
    "print('TrainingError',TrainingError)\n",
    "print('ValidationError',ValidationError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
