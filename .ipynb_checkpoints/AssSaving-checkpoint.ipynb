{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the starter code and some suggested architecture we provide you with. \n",
    "But feel free to do any modifications as you wish or just completely ignore \n",
    "all of them and have your own implementations.\n",
    "\"\"\"\n",
    "# some inspirations were taken from \n",
    "# https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(1242)\n",
    "random.seed(1242)\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self,CostMethod,MaxDepth,Type,Features):\n",
    "        self.CostMethod = CostMethod\n",
    "        self.MaxDepth = MaxDepth\n",
    "        self.Type = Type\n",
    "        self.Features = Features\n",
    "        \"\"\"\n",
    "        TODO: initialization of a decision tree\n",
    "        \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates the entropy given all the labels\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        # assuming binary class\n",
    "        if (len(y)==0):\n",
    "            H = 0\n",
    "        else:\n",
    "            Pc1 = np.sum(y[y==1])/len(y)\n",
    "            Pc0 = 1-Pc1\n",
    "            if ((Pc1==0) |(Pc0==0)):\n",
    "                H=0\n",
    "            else:\n",
    "                H = - (Pc1*np.log(Pc1)+Pc0*np.log(Pc0))\n",
    "        return H\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates information gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(y):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates the gini impurity given all the labels\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates reduction in impurity gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def split(self,SplitVal,SplitIndex, Data):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that return a split of the dataset given an index of the feature and\n",
    "        a threshold for it\n",
    "        \"\"\"\n",
    "        left,right = list(), list()\n",
    "        for DataRow in Data:\n",
    "            if (DataRow[SplitIndex]<=SplitVal):\n",
    "                left.append(DataRow)\n",
    "            else:\n",
    "                right.append(DataRow)\n",
    "        return left,right\n",
    "\n",
    "    def segmenter(self, Node):\n",
    "        \"\"\"\n",
    "        TODO: compute entropy gain for all single-dimension splits,\n",
    "        return the feature and the threshold for the split that\n",
    "        has maximum gain\n",
    "        \"\"\"\n",
    "        X = np.array(Node['Data'])\n",
    "        U,V = np.shape(X)\n",
    "        #print(U,V)\n",
    "        Left,Right = self.split(X[0,0],0,X)\n",
    "        if (self.CostMethod =='Entropy'):\n",
    "            Uleft = len(Left)\n",
    "            if(Uleft ==0):\n",
    "                Hleft =0\n",
    "            else:\n",
    "                Hleft = self.entropy(np.array(Left)[:,-1])\n",
    "            Uright = len(Right)\n",
    "            if(Uright ==0):\n",
    "                Hright =0\n",
    "            else:\n",
    "                Hright = self.entropy(np.array(Right)[:,-1])\n",
    "            Cost = (Uright*Hright+Uleft*Hleft)/U\n",
    "        baselineVal,baselineFeatureIndex,baselineCost,baselineLeft,baselineRight=X[0,0],0,Cost,Left,Right\n",
    "        if (self.Type=='Forest'):\n",
    "            #print(V-1)\n",
    "            FeatureNos = np.array(random.sample(range(0,V-1),int(np.sqrt(V-1))))\n",
    "            #print(FeatureNos)\n",
    "        else:\n",
    "            FeatureNos = np.linspace(0,V-2,V-1,dtype='int32')\n",
    "        for i in range(0,U):\n",
    "            for j in range(0,len(FeatureNos)):\n",
    "                Left,Right = self.split(X[i,FeatureNos[j]],FeatureNos[j],X)\n",
    "                if (self.CostMethod =='Entropy'):\n",
    "                    Uleft = len(Left)\n",
    "                    if(Uleft ==0):\n",
    "                        Hleft =0\n",
    "                    else:\n",
    "                        Hleft = self.entropy(np.array(Left)[:,-1])\n",
    "                    Uright = len(Right)\n",
    "                    if(Uright ==0):\n",
    "                        Hright =0\n",
    "                    else:\n",
    "                        Hright = self.entropy(np.array(Right)[:,-1])\n",
    "                    Cost = (Uright*Hright+Uleft*Hleft)/U\n",
    "                if (Cost<baselineCost):\n",
    "                    #print(X[i,FeatureNos[j]],FeatureNos[j])\n",
    "                    baselineVal,baselineFeatureIndex,baselineCost,baselineLeft,baselineRight=X[i,FeatureNos[j]\n",
    "                                                                                              ],FeatureNos[j],Cost,Left,Right\n",
    "        \n",
    "        Node['Left']   ={'Data':baselineLeft,\n",
    "                         'Label':'Node',\n",
    "                         'Threshold':'nan',\n",
    "                         'Index' : 'nan',\n",
    "                         'Left':'nan',\n",
    "                         'Right' : 'nan',\n",
    "                         'LeafLabel' : 'nan', \n",
    "                        }\n",
    "        Node['Right']  ={'Data':baselineRight,\n",
    "                         'Label':'Node',\n",
    "                         'Threshold':'nan',\n",
    "                         'Index' : 'nan',\n",
    "                         'Left':'nan',\n",
    "                         'Right' : 'nan',\n",
    "                         'LeafLabel' : 'nan', \n",
    "                        }\n",
    "        Node['Threshold'] = baselineVal\n",
    "        Node['Index'] = baselineFeatureIndex\n",
    "        return \n",
    "    def leaf(self,X):\n",
    "        X['Label'] = 'Leaf'\n",
    "        y = np.array(X['Data'])[:,-1]\n",
    "        if (np.count_nonzero(y==1)>np.count_nonzero(y==0)):\n",
    "            X['LeafLabel']=1\n",
    "        else:\n",
    "            X['LeafLabel']=0\n",
    "        return\n",
    "    def CreateBranch(self,X,DepthLevel):\n",
    "        if((DepthLevel+1)>self.MaxDepth):\n",
    "            self.leaf(X)\n",
    "        elif((X['Left']!= 'nan') &(X['Right']!= 'nan') &((len(X['Left'])==0)|(len(X['Right'])==0))):\n",
    "            self.leaf(X)\n",
    "        else:\n",
    "            self.segmenter(X)\n",
    "            if(len(X['Left']['Data'])==0):  # see if there is no segmentation, if so then end the node here\n",
    "                self.leaf(X['Right'])\n",
    "                X['Left']['Label'] = 'Leaf'\n",
    "                X['Left']['LeafLabel'] = X['Right']['LeafLabel']\n",
    "            elif(len(X['Right']['Data'])==0):  # see if there is no segmentation, if so then end the node here\n",
    "                self.leaf(X['Left'])\n",
    "                X['Right']['Label'] = 'Leaf'\n",
    "                X['Right']['LeafLabel'] = X['Left']['LeafLabel']\n",
    "            else:\n",
    "                self.CreateBranch(X['Left'],DepthLevel+1)\n",
    "                self.CreateBranch(X['Right'],DepthLevel+1)\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set. Think about what would be \n",
    "        your stopping criteria\n",
    "        \"\"\"\n",
    "        NodeLevel =0\n",
    "        \n",
    "        Root ={'Data':X,\n",
    "               'Label':'Node',\n",
    "               'Threshold':'nan',\n",
    "               'Index' : 'nan',\n",
    "               'Left':'nan',\n",
    "               'Right' : 'nan',\n",
    "               'LeafLabel' : 'nan', \n",
    "               }\n",
    "        #SplitVal,SplitIndex,LeftGroup,RightGroup = self.segmenter(X,NodeLevel)\n",
    "        self.CreateBranch(Root,NodeLevel)\n",
    "        \n",
    "        self.__repr__(Root,'\\t') # if you want to visualize the tree\n",
    "        \n",
    "        return Root\n",
    "\n",
    "    def predict(self, Node,X):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "        if(Node['Label']=='Leaf'):\n",
    "            Label = Node['LeafLabel']\n",
    "        else:\n",
    "            y = Node['Index']\n",
    "            #print(y.flatten)\n",
    "            y = int(y)\n",
    "            #print(int(y))\n",
    "            if (X[y]>Node['Threshold']):\n",
    "                Label = self.predict(Node['Right'],X)\n",
    "            else:\n",
    "                Label = self.predict(Node['Left'],X)\n",
    "        return Label\n",
    "\n",
    "    def __repr__(self,Node,tab):\n",
    "        \"\"\"\n",
    "        TODO: one way to visualize the decision tree is to write out a __repr__ method\n",
    "        that returns the string representation of a tree. Think about how to visualize \n",
    "        a tree structure. You might have seen this before in CS61A.\n",
    "        \"\"\"\n",
    "        if(Node['Label']=='Node'):\n",
    "            print(tab,'if X[',self.Features[Node['Index']],']>',Node['Threshold'])\n",
    "            self.__repr__(Node['Right'],tab+'\\t')\n",
    "            print(tab,'else')\n",
    "            self.__repr__(Node['Left'],tab+'\\t')\n",
    "        else:\n",
    "            print(tab,'y=',Node['LeafLabel'])\n",
    "        return 0\n",
    "\n",
    "def DataSplit(data, labels, val_size):\n",
    "    num_items = len(data)\n",
    "    assert num_items == len(labels)\n",
    "    assert val_size >= 0\n",
    "    if val_size < 1.0:\n",
    "        val_size = int(num_items * val_size)\n",
    "    train_size = num_items - val_size\n",
    "    idx = np.random.permutation(num_items)\n",
    "    data_train1 = data[idx][:train_size]\n",
    "    label_train = labels[idx][:train_size]\n",
    "    data_val = data[idx][train_size:]\n",
    "    label_val = labels[idx][train_size:]\n",
    "    return data_train1, data_val, label_train, label_val\n",
    "if __name__ == \"__main__\":\n",
    "    #dataset = \"titanic\"\n",
    "    dataset = \"spam\"\n",
    "    Bagging = 'Off'\n",
    "    Validation = 'On' # if off that means we are doing the test data \n",
    "    ClassifierType = 'Tree'\n",
    "    DataFileName = 'SPAMDTTestV1.mat'\n",
    "    Rootdata = scipy.io.loadmat('SPAM25LevelKaggleV3.mat')\n",
    "   # print(Rootdata['Tree'])\n",
    "    Root = Rootdata['Tree']\n",
    "    #return\n",
    "    #FeatureName = []\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data       \n",
    "        data1 =scipy.io.loadmat('TitanicProcessedV2.mat')\n",
    "        X = data1[\"X\"]\n",
    "        y = data1[\"y\"]\n",
    "        Z = data1[\"Z\"]    \n",
    "        y = y.reshape(np.size(y),1)\n",
    "        FeatureName =['pclass','sex','age','sibsp','parch','fare','embarked']# for titanic\n",
    "        # TODO: preprocess titanic dataset\n",
    "        # Notes: \n",
    "        # 1. Some data points are missing their labels\n",
    "        # 2. Some features are not numerical but categorical\n",
    "        # 3. Some values are missing for some features\n",
    "        \n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "        FeatureName = features\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam-dataset/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "        #return\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "    # splitting data into training and validation\n",
    "    if (Validation == 'On'):\n",
    "        ValidationSize = int(len(y)*0.20)\n",
    "        TrainingData,ValidationData,TrainingLabel,ValidationLabel = DataSplit(X,y,ValidationSize)\n",
    "        # converting the data into array for easier preprocessing\n",
    "        TrainingData = np.array(TrainingData)\n",
    "        TrainingLabel = np.array(TrainingLabel)\n",
    "        ValidationData = np.array(ValidationData)\n",
    "        ValidationLabel = np.array(ValidationLabel)\n",
    "        return\n",
    "    else:\n",
    "        TrainingData = np.array(X)\n",
    "        TrainingLabel = np.array(y)\n",
    "        ValidationData = np.array(Z)\n",
    "        ValidationSize = len(Z)\n",
    "    #print(np.shape(TrainingData))\n",
    "    TrainingData = np.concatenate((TrainingData,TrainingLabel.reshape(len(TrainingLabel),1)),axis=1)\n",
    "    #print(np.shape(TrainingData))\n",
    "    #return\n",
    "    #creating an instance of the classifier\n",
    "    if (Bagging=='Off'):\n",
    "       \n",
    "        \n",
    "            MaxDepth = np.linspace(1,1,1)\n",
    "            ValidationError = np.zeros((len(MaxDepth),1),dtype ='float64')\n",
    "            TrainingError = np.zeros((len(MaxDepth),1),dtype ='float64')\n",
    "        \n",
    "            classifier = DecisionTree('Entropy',MaxDepth[0],ClassifierType,FeatureName)\n",
    "            import time\n",
    "            start = time.time()\n",
    "            \n",
    "            ValidationPredicted = np.zeros((ValidationSize,1),dtype ='int64')\n",
    "            TrainingPredicted = np.zeros((len(TrainingLabel),1),dtype ='int64')\n",
    "            for i in range(0,len(ValidationData)):\n",
    "                ValidationPredicted[i] = classifier.predict(Root,ValidationData[i,:])\n",
    "            for i in range(0,len(TrainingLabel)):\n",
    "                TrainingPredicted[i] = classifier.predict(Root,TrainingData[i,:])\n",
    "            if (Validation=='On'):\n",
    "                ValidationError = np.sum(np.abs((ValidationLabel.reshape(ValidationSize,1)\n",
    "                                                    -ValidationPredicted.reshape(ValidationSize,1))\n",
    "                                                   .reshape(ValidationSize,1)))/ValidationSize\n",
    "                TrainingError = np.sum(np.abs((TrainingLabel.reshape(len(TrainingLabel),1)\n",
    "                                            -TrainingPredicted.reshape(len(TrainingLabel),1))\n",
    "                                           .reshape(len(TrainingLabel),1)))/len(TrainingLabel)\n",
    "                end = time.time()\n",
    "                print(end-start,'seconds')\n",
    "                #scipy.io.savemat(DataFileName,{'ValidationError':ValidationError,'TrainingError':TrainingError,'Tree Depth':MaxDepth})\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclamation [[array([[0.]])]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset= scipy.io.loadmat('SPAMDTKaggle.mat')\n",
    "Root = dataset['Tree']\n",
    "features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "print(features[int(Root['Index'])],Root['Threshold'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ValidationData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c7e6951fdbfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValidationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ValidationData' is not defined"
     ]
    }
   ],
   "source": [
    "print(ValidationData[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
